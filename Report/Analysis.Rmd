---
title: "Analysis"
author: "Nina"
date: "05/05/2022"
output: html_document
---

```{r, echo = FALSE, message = FALSE}
source(here::here("Scripts/SetUp.R"))
```

## Modeling 

Some of the models we will use do not work with NAs. To deal with them, we decided to replace the NAs by the median of their variables. 
Concerning missing values in categorical variables, we will eliminate the rows containing NA for these categorical variables. 

```{r, echo = FALSE, message = FALSE}

#Discrete variables 

bands3_reduced2 <- bands3_reduced2 %>%
     mutate(proof_cut=replace_na(proof_cut, median(proof_cut, na.rm=TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(viscosity=replace_na(viscosity,median(viscosity, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2%>%
  mutate(ink_temperature=replace_na(ink_temperature,median(ink_temperature, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(humidity=replace_na(humidity,median(humidity, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(press_speed=replace_na(press_speed,median(press_speed, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(roller_durometer=replace_na(roller_durometer,mean(roller_durometer, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(current_density=replace_na(current_density,median(current_density, na.rm = TRUE)))
bands3_reduced2 <-bands3_reduced2 %>%
  mutate(anode_space_ratio=replace_na(anode_space_ratio,median(anode_space_ratio, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(chrome_content=replace_na(chrome_content,median(chrome_content, na.rm = TRUE)))

#Continous variables 

bands3_reduced2 <- bands3_reduced2 %>%
  mutate(wax=replace_na(wax,median(wax, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(ink_pct=replace_na(ink_pct,median(ink_pct, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(wax=replace_na(wax,median(wax, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(hardener=replace_na(hardener,median(hardener, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(solvent_pct=replace_na(solvent_pct,median(solvent_pct, na.rm = TRUE)))

bands3_reduced2 <- na.omit(bands3_reduced2)
```
Out of the 540 observations in the database, we now have 467, without missing values.
We then convert all categorical variables into a factor. 

```{r, echo = FALSE, message = FALSE}
bands3_reduced2$grain_screened <- as.factor(bands3_reduced2$grain_screened)
bands3_reduced2$paper_type <- as.factor(bands3_reduced2$paper_type)
bands3_reduced2$ink_type <- as.factor(bands3_reduced2$ink_type)
bands3_reduced2$cylinder_type <- as.factor(bands3_reduced2$cylinder_type)
bands3_reduced2$press_type <- as.factor(bands3_reduced2$press_type)
bands3_reduced2$press <- as.factor(bands3_reduced2$press)
bands3_reduced2$cylinder_size <- as.factor(bands3_reduced2$cylinder_size)
bands3_reduced2$band_type <- as.factor(bands3_reduced2$band_type)
```
We can now proceed with our analysis.

### Data splitting and data balancing

<br />

In order to carry out our analysis as accurately as possible, we have split our data set into a training set (**bands3_reduced2.tr**) and a test set (**bands3_reduced2.te**). The training set has 75% of the observations and the test set has the 25% remainder.
<br />
As far as data balancing is concerned, we decided not to do it, as "band" obervations and "noband" ones seem well balanced (respectively 42,2 % and 57,8% as we have seen in the EDA). 

```{r, echo = FALSE, message = FALSE}
set.seed(123) ## for replication purpose
## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(bands3_reduced2), replace=FALSE,
                   size=0.75*nrow(bands3_reduced2))
bands3_reduced2.tr <- bands3_reduced2[index.tr,] ## the training set
bands3_reduced2.te <- bands3_reduced2[-index.tr,] ## the test set
```

<br />
### Random forest

```{r, echo = FALSE, message = FALSE}
set.seed(123)

rf <- randomForest(band_type ~ .,data=bands3_reduced2.tr, importance = TRUE)
predtr <- predict(rf, newdata = bands3_reduced2.tr[-21])
predte <-  predict(rf, newdata= bands3_reduced2.te[-21])
cmtr <-   table(bands3_reduced2.tr[,21], predtr)
cmte <-  table(bands3_reduced2.te[,21], predte)

varImpPlot(rf, cex = 0.65)

print(rf)
```
Here we can see the difference between the descending average of the precision that each variable adds to the model and the descending average of Gini which is a coefficient that shows the contribution of each variable to the homogeneity of nodes and leaves. We notice that some variables have a higher precision importance than their importance based on their Gini score, for our selection of future candidate variables for a simpler model we will make a compromise between these two measures.

So, by making a compromise between the 2 tables, we decided that the variables: "press", "press_speed", "viscosity", "ink_temperature", "press_type" and "hardener", were the most important variables according to the Random Forest model. 

```{r, echo = FALSE, message = FALSE}
#importance(rf)
Acc_tr <- sum(diag(cmtr))/sum(cmtr)
Acc_te <- sum(diag(cmte))/sum(cmte)
```
```{r, echo = FALSE, message = FALSE}
# Confusion Matrix of the Random Forest model on training set 
cmtr

cat("Accuracy of the train set",Acc_tr)
```
The first table is the confusion matrix of the Random Forest model on the training set. Therefore it is normal to have an accuracy of 1 since the model already knows the observations. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix of the Random Forest model on test set
cmte

cat("Accuracy of the test set", Acc_te)
```
Then the second table is the confusion matrix of the random forest model on the test set. We now have an accuracy of 0,81197, which is quite good.


<br />
### CART model 


The next model we are going to use is the Classification and Regression Trees (CART). It is an algorithm where the target variable is fixed or categorical. The algorithm will then identify the “class” in which the target variable is most likely to be found. Here is the tree we obtained.


```{r include=FALSE}
set.seed(123456)
bands <- rpart(band_type ~ ., method = "class", data = bands3_reduced2.tr, control = rpart.control(minsplit = 4,
    cp = 1e-05), model = TRUE)
summary(bands)
```

```{r, echo = FALSE, message = FALSE}
par(pty = "s", mar = c(1, 1, 1, 1))
plot(bands, cex = 1)
text(bands, cex = 0.6)
```

As this tree seems too big and not so easy to interpret, we decided to prune the tree. 

```{r, echo = FALSE, message = FALSE}
printcp(bands)

par(pty = "s")
plotcp(bands)
```

```{r, echo = FALSE, message = FALSE}
cp <- bands$cptable
opt <- which.min(bands$cptable[, "xerror"])
r <- cp[, 4][opt] + cp[, 5][opt]
rmin <- min(seq(1:dim(cp)[1])[cp[, 4] < r])
cp0 <- cp[rmin, 1]
cat("size chosen was", cp[rmin, 2] + 1, "\n")
```

So, the correct size chosen was 6 and below is the final tree we obtained.

```{r include=FALSE}
bands.ct <- prune(bands, cp = 1.01 * cp0)
summary(bands.ct)
```
```{r, echo = FALSE, message = FALSE}
par(mar = c(0.5, 1, 0.5, 1))
plot(bands.ct, branch = 0.4, uniform = TRUE)
text(bands.ct, digits = 3, use.n = TRUE, cex = 0.6)
```

This tree is easier to understand and more meaningful. However, it remains vague. Indeed, the first variable used at the top of the tree is "press" but we know that "press" is a variable with 8 categories. But here, the tree does not indicate the categories of the presses predicting the class "band" or "noband".This is also the case for the variable "press_type" which has 4 categories. 
For the rest of the tree, it seems more useful, and we see that variables used in this tree are quite the same as the ones important according to the Random Forest ("press", press_speed", "viscosity" and "press_type").
So even if this model does not help us to understand exactly why this or that variable is determinant, it still helps us in the choice of the important variables. 

```{r, echo = FALSE, message = FALSE}
bands.pred <- predict(bands.ct, bands3_reduced2.te,type = "class")

confMat <- table(bands3_reduced2.te$band_type,bands.pred)
kable(table(bands3_reduced2.te$band_type, bands.pred), caption = "Prediction table of the CART model")

accuracy <- sum(diag(confMat))/sum(confMat)
print(accuracy)
```
The accuracy of the CART model is 0.70085, which is less than the accuracy of the Random Forest model, but still good and relatively high. 

<br />
### Naïve Bayes

The Naive Bayes model is a simple probabilistic classifier, based on Bayes Theorem. We did a Conditional density Plots with 'usekernel=TRUE', which means that density is used to estimate the class conditional densities of metric predictors. We also set a 'laplace' smoothing (laplace=1). We decided to not include the graphs obtained with this function in the report because it is the same as the variables exploration we did in earlier in the EDA (with "explore_all" function). Moreover, those graphs in the EDA are more complete as they show the distribution of each variables by distinguishing "band" and "noband" observations. 


```{r include=FALSE}
model.nb <- naive_bayes(band_type ~ .,
                       data = bands3_reduced2.tr, usekernel=TRUE, laplace=1)
#par(mfrow=c(2,2))
plot(model.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8),
     prob="conditional")
par(mfrow=c(1,1))
model.nb
```

Here is the confusion matrix on the test data set (model.nb)

```{r, echo = FALSE, warning=FALSE, message=FALSE}
model.nb <- naive_bayes(band_type ~. ,data =bands3_reduced2.tr)
nb_train_predict <- predict(model.nb, 
                            bands3_reduced2.te[ , 
                                              names(bands3_reduced2.te)
                                              !="band_type"])
cfm_nb <- confusionMatrix(nb_train_predict, bands3_reduced2.te$band_type)
cfm_nb                          
```
The accuracy for the train data set is  70.94% (balanced accuracy = 63.44%), which remains good. 

<br />
### K-NN Model

The k Nearest Neighbor Model is a supervised learning method which aimed at classifying a data point into the target class, depending on the features of its nearest data points. Firstly, we used a 2-NN to predict the test set using the training set. 

```{r,echo = FALSE, warning=FALSE, message=FALSE}
set.seed(123)
KNN <- knn3(data=bands3_reduced2.tr, bands3_reduced2.tr$band_type ~ ., k=2)
BT.te.pred <- predict(KNN, newdata = bands3_reduced2.te,type ="class") 

TAB <- table(Obs=bands3_reduced2.te$band_type, Pred= BT.te.pred) # confusion matrix 
TAB
(ACC <- sum(diag(TAB))/sum(TAB)) # accuracy 
```
However, we can see that the accuracy is around 59,83%, which is not very good so we decided to then try with a 3-NN to predict the test set.  

```{r,echo = FALSE, warning=FALSE, message=FALSE}
set.seed(123)
KNN2 <- knn3(data=bands3_reduced2.tr, bands3_reduced2.tr$band_type ~ ., k=3)
BT.te.pred2 <- predict(KNN2, newdata = bands3_reduced2.te,type ="class") 

TAB2 <- table(Obs=bands3_reduced2.te$band_type, Pred= BT.te.pred2) # confusion matrix 
TAB2
(ACC <- sum(diag(TAB2))/sum(TAB2)) # accuracy 
```

We therefore obtained an accuracy of 0,7094017 which is much better. 

<br />
### Neural Network 

The Neural Network model is a type of nonlinear regression model that takes a set of inputs (explanatory variables), transforms and weights these within a set of hidden units and hidden layers to produce a set of outputs or predictions (also transformed).


```{r include=FALSE}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)
nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
nnetFit <- train(band_type ~ ., 
                 data = bands3_reduced2.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 6 hidden layers, with a decay of 0.4. 

The manually written Neural Network model 
```{r include=FALSE}
set.seed(345)
nn4 <- nnet(band_type ~ ., data=bands3_reduced2.tr, size=6, decay = 0.4)
nn4_pred <- predict(nn4, type="class")
tab4 <- table(Obs=bands3_reduced2.tr$band_type, Pred=nn4_pred) # confusion matrix
tab4
(acc4 <- sum(diag(tab4))/sum(tab4)) # accuracy
```

Here it says that the accuracy is 79,71%.
```{r, echo = FALSE, warning=FALSE, message=FALSE}
# Confusion Matrix
confusionMatrix(data=as.factor(nn4_pred), reference = bands3_reduced2.tr$band_type)
```


### Logistic regression

```{r, echo = FALSE, warning=FALSE, message=FALSE}
glm.fit <- glm(formula = band_type ~ grain_screened + paper_type + ink_type + cylinder_type + press_type + press + cylinder_size + proof_cut + viscosity + ink_temperature + humidity + press_speed + ink_pct + wax + hardener + roller_durometer + current_density + anode_space_ratio + chrome_content, family = binomial, data = bands3_reduced2)
summary(glm.fit)
coef(glm.fit)
```


Stepwise variable selection with backward selection 
```{r include=FALSE}
glm.sel <- step(glm.fit)
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(glm.sel)
```
The stepwise selection has eliminated several variables: "grain_screened", "paper_type", "cylinder_size", "proof_cut", "ink_temperature", "humidity", "hardener", "roller_durometer" and "chrome_content". 
We can see that the probability of having "noband" is: 
- smaller with a COVER ink_type and higher with an UNCOATED one
- smaller with press 815, 816 and 821 
- increasing a little with press_speed 

Predictions and confusion matrix
```{r, echo = FALSE, warning=FALSE, message=FALSE}
prob.te <- predict(glm.sel, newdata = bands3_reduced2.te, type="response")
pred.te <- ifelse(prob.te >= 0.5, 1, 0)
TAB6 <- table(Pred=pred.te, Obs=bands3_reduced2.te$band_type)
(ACC <- sum(diag(TAB6))/sum(TAB6)) # accuracy 
```
<br />
The accuracy of the logistic regression is 0.76068. 
<br />
```{r, echo = FALSE, warning=FALSE, message=FALSE}
boxplot(prob.te~bands3_reduced2.te$band_type)
```
<br />
<br />
We distinguish well the "band" and the "noband" boxplot. Indeed, most of the probabilities remain above 0,7 for the "noband" observations. However, probabilities are included between 0,1 and 0,8 for the "band" observations, so it's not very helpful. A good model would have two well separated boxplots, well away from 0.5.

### SVM

```{r, echo = FALSE, warning=FALSE, message=FALSE}
model.svm <- svm(band_type ~ ., data = bands3_reduced2.tr, method = "C-classification", 
                   kernel = "radial", cost = 10, gamma = 0.1)
summary(model.svm)
```
```{r}
test_pred <- predict(model.svm, newdata = bands3_reduced2.te)
test_pred

confusionMatrix(table(test_pred, bands3_reduced2.te$band_type))
```

```{r include=FALSE}
model.svm$index
```

```{r include=FALSE}
model.svm$SV
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
kable(table(true=bands3_reduced2.te$band_type, predicted=predict(model.svm, bands3_reduced2.te, decision.values = TRUE)), caption = "Prediction table.")
```
### Unsupervised learning analysis 

#### Cluster Analysis 

```{r echo=FALSE, results='hide',message=FALSE, warning = FALSE}
# New Dataframe: Select only the numerical values:
bands3_reduced2.num <- select_if(bands3_reduced2, is.numeric)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
row.names(bands3_reduced2.num) <- paste("M", c(1:nrow(bands3_reduced2.num)), sep="") # row names are used after
# scaling the data 
bands3_reduced2.num <- scale(bands3_reduced2.num)
```


#### Agglomerative Hierarchical Clustering (AGNES) with Manhattan distance

```{r echo=FALSE, results='hide',message=FALSE, warning = FALSE, fig.show='hide'}
library(reshape2)
set.seed(123)
# matrix of Manhattan distances 
bands.d <- dist(bands3_reduced2.num, method = "manhattan") 
# create a data frame of the distances in long format
bands.melt <- melt(as.matrix(bands.d))
ggplot(data = bands.melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() 
#dendrogram using a complete linkage.
bands.hc <- hclust(bands.d, method = "complete")
plot(bands.hc, hang=-1)
#cut the tree to 2 clusters
plot(bands.hc, hang=-1)
rect.hclust(bands.hc, k=2)
bands.clust <- cutree(bands.hc, k=2)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, out.width = "70%", fig.align="center", fig.width=10,fig.height=4}
#number of clusters
fviz_nbclust(bands3_reduced2.num,
             hcut, hc_method="complete",
             hc_metric="manhattan",
             method = "silhouette", 
             k.max = 25, verbose = FALSE)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width=13,fig.height=10, out.width = "70%", fig.align="center"}
#Interpretation of the clusters
bands.comp <- data.frame(bands3_reduced2.num, 
                            Clust=factor(bands.clust)
                            ,Id=row.names(bands3_reduced2.num))
bands.df <- melt(bands.comp, id=c("Id", "Clust"))
ggplot(bands.df, aes(y=value, group=Clust, fill=Clust)) +
  geom_boxplot() +
  facet_wrap(~variable, scale="free")
```
Using the dendrogram with complete linkage on Manhattan distance with the silhouette method, we identified k=2 as the optimal number of clusters.
By analyzing this clusters, we can 


# Evaluation : Model selection

- Random Forest : **81.2%**
- CART : **70.1%**
- Naïve Bayes : **70.9%**
- K-NN : **70.9%** 
- Neural Network : **79.7%**
- Logistic Regression : **76.1%**
- SVM : **76.9**
- 
