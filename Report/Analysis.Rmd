---
title: "Analysis"
author: "Nina"
date: "05/05/2022"
output: html_document
---

```{r, echo = FALSE, message = FALSE}
source(here::here("Scripts/SetUp.R"))
```


### Replace the NAs by the median of each column's variable 
Some of the models we will use do not work with NAs. To deal with them, we decided to replace the NAs by the median of their variables. 
Concerning missing values in categorical variables, we will eliminate the rows containing NA for these categorical variables. 
```{r}

#Discrete variables 

bands3_reduced2 <- bands3_reduced2 %>%
     mutate(proof_cut=replace_na(proof_cut, median(proof_cut, na.rm=TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(viscosity=replace_na(viscosity,median(viscosity, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2%>%
  mutate(ink_temperature=replace_na(ink_temperature,median(ink_temperature, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(humidity=replace_na(humidity,median(humidity, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(press_speed=replace_na(press_speed,median(press_speed, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(roller_durometer=replace_na(roller_durometer,mean(roller_durometer, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(current_density=replace_na(current_density,median(current_density, na.rm = TRUE)))
bands3_reduced2 <-bands3_reduced2 %>%
  mutate(anode_space_ratio=replace_na(anode_space_ratio,median(anode_space_ratio, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(chrome_content=replace_na(chrome_content,median(chrome_content, na.rm = TRUE)))

#Continous variables 

bands3_reduced2 <- bands3_reduced2 %>%
  mutate(wax=replace_na(wax,median(wax, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(ink_pct=replace_na(ink_pct,median(ink_pct, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(wax=replace_na(wax,median(wax, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(hardener=replace_na(hardener,median(hardener, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(solvent_pct=replace_na(solvent_pct,median(solvent_pct, na.rm = TRUE)))

bands3_reduced2 <- na.omit(bands3_reduced2)
```
Out of the 540 observations in the database, we now have 467, without missing values.
We now convert all categorical variables into a factor. 

```{r}
bands3_reduced2$grain_screened <- as.factor(bands3_reduced2$grain_screened)
bands3_reduced2$paper_type <- as.factor(bands3_reduced2$paper_type)
bands3_reduced2$ink_type <- as.factor(bands3_reduced2$ink_type)
bands3_reduced2$cylinder_type <- as.factor(bands3_reduced2$cylinder_type)
bands3_reduced2$press_type <- as.factor(bands3_reduced2$press_type)
bands3_reduced2$press <- as.factor(bands3_reduced2$press)
bands3_reduced2$cylinder_size <- as.factor(bands3_reduced2$cylinder_size)
bands3_reduced2$band_type <- as.factor(bands3_reduced2$band_type)
```
We can now proceed with our analysis.

## Splitting the data into Traning set and Test set 

```{r}
set.seed(123) ## for replication purpose
## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(bands3_reduced2), replace=FALSE,
                   size=0.75*nrow(bands3_reduced2))
bands3_reduced2.tr <- bands3_reduced2[index.tr,] ## the training set
bands3_reduced2.te <- bands3_reduced2[-index.tr,] ## the test set
```

## Balancing the Training set : PAS BESOIN POUR L'INSTANT

```{r}
#band1 <- min(table(bands3_reduced.tr$band_type)) ## 171
## the "band" cases
#data.tr.band1 <- filter(bands3_reduced.tr, band_type=="band")
## the "noband" cases
#data.tr.band2 <- filter(bands3_reduced.tr, band_type=="noband")

## sub-sample 171 instances from the number of "noband" cases
#index.band2 <- sample(size=band1, 
#                           x=1:nrow(data.tr.band2), 
#                           replace=FALSE)

  
## Bind all the "Central Andes" and the sub-sampled "Central Pyrenees" 
## and the sub-sampled "Sierra de Guadarrama"
#bands3_reduced.tr <- data.frame(rbind(data.tr.band1,
#                                     data.tr.band2[index.band2,])) 
## The cases are now balanced
#table(bands3_reduced.tr$band_type)
```



## 1. Random forest

```{r}
set.seed(123)

rf <- randomForest(band_type ~ .,data=bands3_reduced2.tr, importance = TRUE)
predtr <- predict(rf, newdata = bands3_reduced2.tr[-21])
predte <-  predict(rf, newdata= bands3_reduced2.te[-21])
cmtr <-   table(bands3_reduced2.tr[,21], predtr)
cmte <-  table(bands3_reduced2.te[,21], predte)

varImpPlot(rf, cex = 0.65)

print(rf)
```
Here we can see the difference between the descending average of the precision that each variable adds to the model and the descending average of Gini which is a coefficient that shows the contribution of each variable to the homogeneity of nodes and leaves. We notice that some variables have a higher precision importance than their importance based on their Gini score, for our selection of future candidate variables for a simpler model we will make a compromise between these two measures.

```{r}
#importance(rf)
Acc_tr <- sum(diag(cmtr))/sum(cmtr)
Acc_te <- sum(diag(cmte))/sum(cmte)
```
```{r}
# Confusion Matrix of the Random Forest model on training set 
cmtr

cat("Accuracy of the train set",Acc_tr)
```
The first table is the confusion matrix of the Random Forest model on the training set. Therefore it is normal to have an accuracy of 1 since the model already knows the observations. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix of the Random Forest model on test set
cmte

cat("Accuracy of the test set", Acc_te)
```
Then the second table is the confusion matrix of the random forest model on the test set. We now have an accuracy of 0,81197, which is quite good.



## 2. CART model 

```{r}
set.seed(123456)
bands <- rpart(band_type ~ ., method = "class", data = bands3_reduced2.tr, control = rpart.control(minsplit = 4,
    cp = 1e-05), model = TRUE)
summary(bands)
```

```{r}
par(pty = "s", mar = c(1, 1, 1, 1))
plot(bands, cex = 1)
text(bands, cex = 0.6)
```

```{r}
printcp(bands)

par(pty = "s")
plotcp(bands)
```

```{r}
cp <- bands$cptable
opt <- which.min(bands$cptable[, "xerror"])
r <- cp[, 4][opt] + cp[, 5][opt]
rmin <- min(seq(1:dim(cp)[1])[cp[, 4] < r])
cp0 <- cp[rmin, 1]
cat("size chosen was", cp[rmin, 2] + 1, "\n")
```

```{r}
bands.ct <- prune(bands, cp = 1.01 * cp0)
summary(bands.ct)
```
```{r}
par(mar = c(0.5, 1, 0.5, 1))
plot(bands.ct, branch = 0.4, uniform = TRUE)
text(bands.ct, digits = 3, use.n = TRUE, cex = 0.6)
```

```{r}
bands.pred <- predict(bands.ct, bands3_reduced2.te,type = "class")

confMat <- table(bands3_reduced2.te$band_type,bands.pred)
confMat
accuracy <- sum(diag(confMat))/sum(confMat)
print(accuracy)
```
The accuracy of the CART model is 0.70085, which is less than the accuracy of the Random Forest model. 

## 3. NaÃ¯ve Bayes

#Conditional density Plots with 'usekernel=TRUE' 
```{r}
model.nb <- naive_bayes(band_type ~ .,
                       data = bands3_reduced2.tr, usekernel=TRUE, laplace=1)
#par(mfrow=c(2,2))
plot(model.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8),
     prob="conditional")
par(mfrow=c(1,1))
model.nb
```


#Conditional density plots with 'uskernel=FALSE'. 
It means that Gaussian distribution is applied to 'numeric' variable. 
```{r}
model.nb2 <- naive_bayes(band_type ~ .,
                       data = bands3_reduced2.tr, usekernel=FALSE, laplace=1) 
#par(mfrow=c(2,2))
plot(model.nb2, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8), prob="conditional")
par(mfrow=c(1,1))
model.nb2
```

# Confusion matrix train data set (model.nb)
```{r echo = FALSE, warning=FALSE, message=FALSE}
model.nb <- naive_bayes(band_type ~. ,data =bands3_reduced2.tr)
nb_train_predict <- predict(model.nb, 
                            bands3_reduced2.te[ , 
                                              names(bands3_reduced2.te)
                                              !="band_type"])
cfm_nb <- confusionMatrix(nb_train_predict, bands3_reduced2.te$band_type)
cfm_nb                          
```
The accuracy for the train data set is  70.94% (balanced accuracy = 63.44%).

# Confusion matrix train data set (model.nb2)
```{r echo = FALSE, warning=FALSE, message=FALSE}
model.nb2 <- naive_bayes(band_type ~. ,data =bands3_reduced2.tr)
nb_train_predict <- predict(model.nb2, 
                            bands3_reduced2.te[ , 
                                              names(bands3_reduced2.te)
                                              !="band_type"])
cfm_nb <- confusionMatrix(nb_train_predict, bands3_reduced2.te$band_type)
cfm_nb                          
```
The accuracy for the train data set is  70.94% (balanced accuracy = 63.44%).

## 4. K-NN Model

We use a 2-NN to predict the test set using the training set
```{R}
set.seed(123)
KNN <- knn3(data=bands3_reduced2.tr, bands3_reduced2.tr$band_type ~ ., k=2)
BT.te.pred <- predict(KNN, newdata = bands3_reduced2.te,type ="class") 

TAB <- table(Obs=bands3_reduced2.te$band_type, Pred= BT.te.pred) # confusion matrix 
TAB
(ACC <- sum(diag(TAB))/sum(TAB)) # accuracy 
```
The accuracy is around 59,83%.


## 5. Neural Network 

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)
nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
nnetFit <- train(band_type ~ ., 
                 data = bands3_reduced2.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)
```

```{r}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 6 hidden layers, with a decay of 0.4. 

The manually written Neural Network model 
```{r}
set.seed(345)
nn4 <- nnet(band_type ~ ., data=bands3_reduced2.tr, size=6, decay = 0.4)
nn4_pred <- predict(nn4, type="class")
tab4 <- table(Obs=bands3_reduced2.tr$band_type, Pred=nn4_pred) # confusion matrix
tab4
(acc4 <- sum(diag(tab4))/sum(tab4)) # accuracy
```

Here it says that the accuracy is 79,71%.
```{r}
# Confusion Matrix
confusionMatrix(data=as.factor(nn4_pred), reference = bands3_reduced.tr$band_type)
```


## 6. Logistic regression

```{r}
glm.fit <- glm(formula = band_type ~ grain_screened + paper_type + ink_type + cylinder_type + press_type + press + cylinder_size + proof_cut + viscosity + ink_temperature + humidity + press_speed + ink_pct + wax + hardener + roller_durometer + current_density + anode_space_ratio + chrome_content, family = binomial, data = bands3_reduced2)
summary(glm.fit)
coef(glm.fit)
```

# Confusion matrix (marche pas)
```{r}
glm_pred <- predict(glm.fit, type="class")
glm.fit.pred <- table(Obs=bands3_reduced2$band_type, Pred=glm_pred) 
# confusion matrix
glm.fit.pred
(acc.glm <- sum(diag(glm.fit.pred))/sum(glm.fit.pred)) # accuracy
```


## 7. SVM

```{r}
library(e1071)
model.train <- svm(band_type ~ ., data = bands3_reduced2.tr, method = "C-classification", 
                   kernel = "radial", cost = 10, gamma = 0.1)
summary(model.train)
```

```{r}
model.train$index
```

```{r}
model.train$SV
```

```{r}
kable(table(true=bands3_reduced2.te$band_type, predicted=predict(model.train, bands3_reduced2.te, decision.values = TRUE)), caption = "Prediction table.")
```
# By using repeated cross-validation
```{r}
model_svm <- caret::train(band_type ~ .,
                          data = bands3_reduced2.tr, 
                          method = "svmRadialCost",
                          preProcess = "range",
                          trace = FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE))
model_svm
```

```{r}
model_svm$bestTune
```

```{r}
plot(model_svm)
```

```{r}
C <- c(0.25, 0.1, 0.5, 1, 10, 100)
sigma <- c(0.0001, 0.001, 0.01, 0.1, 1)
gr.radial<-expand.grid(C = C, sigma = sigma)
model_svm.1<-caret::train(band_type ~ .,
                          data = bands3_reduced2.tr,
                          method = "svmRadial",
                          preProcess = "range",
                          trace=FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE),
                          tuneGrid=gr.radial)
model_svm.1
```

```{r}
model_svm.1$bestTune
```

```{r}
plot(model_svm.1)
```

```{r}
kable(table(true=bands3_reduced2.te$band_type, predicted=predict(model_svm.1, bands3_reduced2.te, decision.values = TRUE)), caption = "Prediction table.")
```
