---
title: "Analysis"
author: "Nina"
date: "05/05/2022"
output: html_document
---

```{r, echo = FALSE, message = FALSE}
source(here::here("Scripts/SetUp.R"))
```
--> faire une phrase qui explique pq pas de balance data

## Modeling 

Some of the models we will use do not work with NAs. To deal with them, we decided to replace the NAs by the median of their variables. 
Concerning missing values in categorical variables, we will eliminate the rows containing NA for these categorical variables. 

```{r, echo = FALSE, message = FALSE}

#Discrete variables 

bands3_reduced2 <- bands3_reduced2 %>%
     mutate(proof_cut=replace_na(proof_cut, median(proof_cut, na.rm=TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(viscosity=replace_na(viscosity,median(viscosity, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2%>%
  mutate(ink_temperature=replace_na(ink_temperature,median(ink_temperature, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(humidity=replace_na(humidity,median(humidity, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(press_speed=replace_na(press_speed,median(press_speed, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(roller_durometer=replace_na(roller_durometer,mean(roller_durometer, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(current_density=replace_na(current_density,median(current_density, na.rm = TRUE)))
bands3_reduced2 <-bands3_reduced2 %>%
  mutate(anode_space_ratio=replace_na(anode_space_ratio,median(anode_space_ratio, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(chrome_content=replace_na(chrome_content,median(chrome_content, na.rm = TRUE)))

#Continous variables 

bands3_reduced2 <- bands3_reduced2 %>%
  mutate(wax=replace_na(wax,median(wax, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(ink_pct=replace_na(ink_pct,median(ink_pct, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(wax=replace_na(wax,median(wax, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(hardener=replace_na(hardener,median(hardener, na.rm = TRUE)))
bands3_reduced2 <- bands3_reduced2 %>%
  mutate(solvent_pct=replace_na(solvent_pct,median(solvent_pct, na.rm = TRUE)))

bands3_reduced2 <- na.omit(bands3_reduced2)
```
Out of the 540 observations in the database, we now have 467, without missing values.
We then convert all categorical variables into a factor. 

```{r, echo = FALSE, message = FALSE}
bands3_reduced2$grain_screened <- as.factor(bands3_reduced2$grain_screened)
bands3_reduced2$paper_type <- as.factor(bands3_reduced2$paper_type)
bands3_reduced2$ink_type <- as.factor(bands3_reduced2$ink_type)
bands3_reduced2$cylinder_type <- as.factor(bands3_reduced2$cylinder_type)
bands3_reduced2$press_type <- as.factor(bands3_reduced2$press_type)
bands3_reduced2$press <- as.factor(bands3_reduced2$press)
bands3_reduced2$cylinder_size <- as.factor(bands3_reduced2$cylinder_size)
bands3_reduced2$band_type <- as.factor(bands3_reduced2$band_type)
```
We can now proceed with our analysis.

### Data splitting and data balancing

```{r, echo = FALSE, message = FALSE}
set.seed(123) ## for replication purpose
## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(bands3_reduced2), replace=FALSE,
                   size=0.75*nrow(bands3_reduced2))
bands3_reduced2.tr <- bands3_reduced2[index.tr,] ## the training set
bands3_reduced2.te <- bands3_reduced2[-index.tr,] ## the test set
```


### Random forest

```{r, echo = FALSE, message = FALSE}
set.seed(123)

rf <- randomForest(band_type ~ .,data=bands3_reduced2.tr, importance = TRUE)
predtr <- predict(rf, newdata = bands3_reduced2.tr[-21])
predte <-  predict(rf, newdata= bands3_reduced2.te[-21])
cmtr <-   table(bands3_reduced2.tr[,21], predtr)
cmte <-  table(bands3_reduced2.te[,21], predte)

varImpPlot(rf, cex = 0.65)

print(rf)
```
Here we can see the difference between the descending average of the precision that each variable adds to the model and the descending average of Gini which is a coefficient that shows the contribution of each variable to the homogeneity of nodes and leaves. We notice that some variables have a higher precision importance than their importance based on their Gini score, for our selection of future candidate variables for a simpler model we will make a compromise between these two measures.

```{r, echo = FALSE, message = FALSE}
#importance(rf)
Acc_tr <- sum(diag(cmtr))/sum(cmtr)
Acc_te <- sum(diag(cmte))/sum(cmte)
```
```{r, echo = FALSE, message = FALSE}
# Confusion Matrix of the Random Forest model on training set 
cmtr

cat("Accuracy of the train set",Acc_tr)
```
The first table is the confusion matrix of the Random Forest model on the training set. Therefore it is normal to have an accuracy of 1 since the model already knows the observations. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix of the Random Forest model on test set
cmte

cat("Accuracy of the test set", Acc_te)
```
Then the second table is the confusion matrix of the random forest model on the test set. We now have an accuracy of 0,81197, which is quite good.



### CART model 

```{r include=FALSE}
set.seed(123456)
bands <- rpart(band_type ~ ., method = "class", data = bands3_reduced2.tr, control = rpart.control(minsplit = 4,
    cp = 1e-05), model = TRUE)
summary(bands)
```

```{r, echo = FALSE, message = FALSE}
par(pty = "s", mar = c(1, 1, 1, 1))
plot(bands, cex = 1)
text(bands, cex = 0.6)
```

```{r, echo = FALSE, message = FALSE}
printcp(bands)

par(pty = "s")
plotcp(bands)
```

```{r, echo = FALSE, message = FALSE}
cp <- bands$cptable
opt <- which.min(bands$cptable[, "xerror"])
r <- cp[, 4][opt] + cp[, 5][opt]
rmin <- min(seq(1:dim(cp)[1])[cp[, 4] < r])
cp0 <- cp[rmin, 1]
cat("size chosen was", cp[rmin, 2] + 1, "\n")
```

```{r include=FALSE}
bands.ct <- prune(bands, cp = 1.01 * cp0)
summary(bands.ct)
```
```{r, echo = FALSE, message = FALSE}
par(mar = c(0.5, 1, 0.5, 1))
plot(bands.ct, branch = 0.4, uniform = TRUE)
text(bands.ct, digits = 3, use.n = TRUE, cex = 0.6)
```

```{r, echo = FALSE, message = FALSE}
bands.pred <- predict(bands.ct, bands3_reduced2.te,type = "class")

confMat <- table(bands3_reduced2.te$band_type,bands.pred)
kable(table(bands3_reduced2.te$band_type, bands.pred), caption = "Prediction table of the CART model")

accuracy <- sum(diag(confMat))/sum(confMat)
print(accuracy)
```
The accuracy of the CART model is 0.70085, which is less than the accuracy of the Random Forest model. 

### NaÃ¯ve Bayes

We did a Conditional density Plots with 'usekernel=TRUE', which means that ...but 

```{r include=FALSE}
model.nb <- naive_bayes(band_type ~ .,
                       data = bands3_reduced2.tr, usekernel=TRUE, laplace=1)
#par(mfrow=c(2,2))
plot(model.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8),
     prob="conditional")
par(mfrow=c(1,1))
model.nb
```

Here is the confusion matrix on the test data set (model.nb)

```{r, echo = FALSE, warning=FALSE, message=FALSE}
model.nb <- naive_bayes(band_type ~. ,data =bands3_reduced2.tr)
nb_train_predict <- predict(model.nb, 
                            bands3_reduced2.te[ , 
                                              names(bands3_reduced2.te)
                                              !="band_type"])
cfm_nb <- confusionMatrix(nb_train_predict, bands3_reduced2.te$band_type)
cfm_nb                          
```
The accuracy for the train data set is  70.94% (balanced accuracy = 63.44%).


### K-NN Model

We use a 2-NN to predict the test set using the training set
```{r,echo = FALSE, warning=FALSE, message=FALSE}
set.seed(123)
KNN <- knn3(data=bands3_reduced2.tr, bands3_reduced2.tr$band_type ~ ., k=2)
BT.te.pred <- predict(KNN, newdata = bands3_reduced2.te,type ="class") 

TAB <- table(Obs=bands3_reduced2.te$band_type, Pred= BT.te.pred) # confusion matrix 
TAB
(ACC <- sum(diag(TAB))/sum(TAB)) # accuracy 
```
The accuracy is around 59,83%.


## 5. Neural Network 

```{r, echo = FALSE, warning=FALSE, message=FALSE}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)
nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
nnetFit <- train(band_type ~ ., 
                 data = bands3_reduced2.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 6 hidden layers, with a decay of 0.4. 

The manually written Neural Network model 
```{r include=FALSE}
set.seed(345)
nn4 <- nnet(band_type ~ ., data=bands3_reduced2.tr, size=6, decay = 0.4)
nn4_pred <- predict(nn4, type="class")
tab4 <- table(Obs=bands3_reduced2.tr$band_type, Pred=nn4_pred) # confusion matrix
tab4
(acc4 <- sum(diag(tab4))/sum(tab4)) # accuracy
```

Here it says that the accuracy is 79,71%.
```{r, echo = FALSE, warning=FALSE, message=FALSE}
# Confusion Matrix
confusionMatrix(data=as.factor(nn4_pred), reference = bands3_reduced.tr$band_type)
```


### Logistic regression

```{r, echo = FALSE, warning=FALSE, message=FALSE}
glm.fit <- glm(formula = band_type ~ grain_screened + paper_type + ink_type + cylinder_type + press_type + press + cylinder_size + proof_cut + viscosity + ink_temperature + humidity + press_speed + ink_pct + wax + hardener + roller_durometer + current_density + anode_space_ratio + chrome_content, family = binomial, data = bands3_reduced2)
summary(glm.fit)
coef(glm.fit)
```


Stepwise variable selection with backward selection 
```{r include=FALSE}
glm.sel <- step(glm.fit)
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(glm.sel)
```
The stepwise selection has eliminated several variables: "grain_screened", "paper_type", "cylinder_size", "proof_cut", "ink_temperature", "humidity", "hardener", "roller_durometer" and "chrome_content". 
We can see that the probability of having "noband" is: 
- smaller with a COVER ink_type and higher with an UNCOATED one
- smaller with press 815, 816 and 821 
- increasing a little with press_speed 

Predictions and confusion matrix
```{r, echo = FALSE, warning=FALSE, message=FALSE}
prob.te <- predict(glm.sel, newdata = bands3_reduced2.te, type="response")
pred.te <- ifelse(prob.te >= 0.5, 1, 0)
TAB6 <- table(Pred=pred.te, Obs=bands3_reduced2.te$band_type)
(ACC <- sum(diag(TAB6))/sum(TAB6)) # accuracy 
```
<br />
The accuracy of the logistic regression is 0.76068. 
<br />
```{r, echo = FALSE, warning=FALSE, message=FALSE}
boxplot(prob.te~bands3_reduced2.te$band_type)
```
<br />
<br />
We distinguish well the "band" and the "noband" boxplot. Indeed, most of the probabilities remain above 0,7 for the "noband" observations. However, probabilities are included between 0,1 and 0,8 for the "band" observations, so it's not very helpful. A good model would have two well separated boxplots, well away from 0.5.

### SVM

```{r, echo = FALSE, warning=FALSE, message=FALSE}
model.train <- svm(band_type ~ ., data = bands3_reduced2.tr, method = "C-classification", 
                   kernel = "radial", cost = 10, gamma = 0.1)
summary(model.train)
```

```{r include=FALSE}
model.train$index
```

```{r include=FALSE}
model.train$SV
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
kable(table(true=bands3_reduced2.te$band_type, predicted=predict(model.train, bands3_reduced2.te, decision.values = TRUE)), caption = "Prediction table.")
```
# By using repeated cross-validation
```{r}
model_svm <- caret::train(band_type ~ .,
                          data = bands3_reduced2.tr, 
                          method = "svmRadialCost",
                          preProcess = "range",
                          trace = FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE))
model_svm
```

```{r}
model_svm$bestTune
```

```{r}
plot(model_svm)
```

```{r}
C <- c(0.25, 0.1, 0.5, 1, 10, 100)
sigma <- c(0.0001, 0.001, 0.01, 0.1, 1)
gr.radial<-expand.grid(C = C, sigma = sigma)
model_svm.1<-caret::train(band_type ~ .,
                          data = bands3_reduced2.tr,
                          method = "svmRadial",
                          preProcess = "range",
                          trace=FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE),
                          tuneGrid=gr.radial)
model_svm.1
```

```{r}
model_svm.1$bestTune
```

```{r}
plot(model_svm.1)
```

```{r}
kable(table(true=bands3_reduced2.te$band_type, predicted=predict(model_svm.1, bands3_reduced2.te, decision.values = TRUE)), caption = "Prediction table.")
```
